import streamlit as st
from groq import Groq
import requests
import io
from PIL import Image

# --- PAGE CONFIG & SPACE THEME ---
st.set_page_config(page_title="Genis Pro 1.2", page_icon="ðŸš€", layout="centered")

# Custom CSS for the Space Background and Glow
st.markdown("""
    <style>
    .stApp {
        background: radial-gradient(ellipse at bottom, #1B2735 0%, #090A0F 100%);
        color: #ffffff;
    }
    /* Animated stars effect */
    @keyframes move-twinkle {
        from {background-position:0 0;}
        to {background-position:-10000px 5000px;}
    }
    .stApp::before {
        content: "";
        position: absolute;
        top: 0; left: 0; right: 0; bottom: 0;
        background: transparent url('https://www.transparenttextures.com/patterns/stardust.png') repeat top center;
        z-index: -1;
        opacity: 0.5;
        animation: move-twinkle 200s linear infinite;
    }
    h1 { color: #00d4ff !important; text-shadow: 0 0 10px #00d4ff; }
    </style>
    """, unsafe_allow_html=True)

st.title("ðŸš€ Genis Pro 1.2")
st.caption("Powered by BotDevelopmentAI")

# --- API CLIENTS ---
client = Groq(api_key=st.secrets["GROQ_API_KEY"])
HF_API_URL = "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell"
headers = {"Authorization": f"Bearer {st.secrets['HF_TOKEN']}"}

# --- INITIALIZE HISTORY ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system", 
            "content": (
                "You are Genis Pro 1.2, developed by BotDevelopmentAI. "
                "If asked about your creator, always say 'I was made by BotDevelopmentAI.' "
                "You have a built-in image generator called 'SmartBot Ludy'. "
                "Never mention Meta, Llama, or Hugging Face. If a user asks to 'generate' or 'draw' "
                "an image, acknowledge that you are using SmartBot Ludy to do it."
            )
        }
    ]

# --- IMAGE GENERATION FUNCTION ---
def generate_image(prompt):
    response = requests.post(HF_API_URL, headers=headers, json={"inputs": prompt})
    return response.content

# --- CHAT INTERFACE ---
for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

if prompt := st.chat_input("Ask Genis or request an image..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Check if user wants an image
    if any(word in prompt.lower() for word in ["generate image", "draw", "create a picture", "make an image"]):
        with st.chat_message("assistant"):
            with st.spinner("SmartBot Ludy is painting..."):
                try:
                    img_bytes = generate_image(prompt)
                    image = Image.open(io.BytesIO(img_bytes))
                    st.image(image, caption=f"Generated by SmartBot Ludy")
                    st.session_state.messages.append({"role": "assistant", "content": f"I have generated that image for you using SmartBot Ludy."})
                except Exception as e:
                    st.error("SmartBot Ludy is currently busy. Try again in a moment!")
    else:
        # Standard Text Response
        with st.chat_message("assistant"):
            stream = client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=st.session_state.messages,
                stream=True,
            )
            response_text = st.write_stream(stream)
            st.session_state.messages.append({"role": "assistant", "content": response_text})
